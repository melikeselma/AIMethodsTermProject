# ğŸ° Multi-Armed Bandit Term Project

This project investigates the **Multi-Armed Bandit (MAB)** problem and evaluates various strategies to balance the trade-off between **exploration and exploitation**. It simulates both stationary and non-stationary environments, enhancing classical algorithms for improved adaptability.

---

## ğŸ§© 1. Problem Definition

The MAB problem models sequential decision-making under uncertainty, where an agent must choose among multiple options (bandits) with stochastic rewards.  
The challenge:  
> Maximize total reward by balancing **exploitation** of known good options vs. **exploration** of less-tried ones.

Implemented strategies:
- **Îµ-Greedy** (static and adaptive)
- **UCB (Upper Confidence Bound)**
- **Thompson Sampling**
- Enhanced versions of the above (smart reward conditions, meta-agents)

---

## ğŸ” 2. Research Background

- **Thompson Sampling** is a Bayesian approach that samples from posterior distributions and chooses actions accordingly.
- Compared to Îµ-Greedy (fixed randomness), Thompson Sampling dynamically adapts to feedback.
- Widely used in **online learning**, **advertising**, **recommendation engines**, and **healthcare optimization**.

> â€œThompson Sampling achieves low regret while being lightweight and scalable.â€ â€” *StatQuest, DeepMind*

ğŸ“š Key References:
- Shipra Agrawal & Navin Goyal (2012)
- DeepMind Bandit Learning (2021)
- Sutton & Barto (2018)
- Lilian Weng's Blog on MABs

---

## âš™ï¸ 3. Environment and Libraries

- **Environment:** Python 3.11, Jupyter Notebook  
- **Libraries Used:** `numpy`, `matplotlib`, `random`, `tqdm`

---

## ğŸš€ 4. Implemented Methods

### âœ… Enhanced Algorithms:
- **Adaptive Îµ-Greedy:** `Îµ(t) = 1 / âˆšt` for dynamic exploration
- **Smart Thompson Sampling:** Rewards counted as success only if they exceed current expectations
- **Strategy Switching Agent:** Dynamically selects best strategy based on recent performance

### ğŸŒ Environment:
- **Non-stationary bandits:** Mean rewards shift every 500 steps
- Tests both **static** and **changing** reward distributions

---

## ğŸ§ª 5. Experiments

- **Bandits:** 5 bandits with randomized mean rewards  
- **Steps:** 5000 actions per strategy  
- **Evaluation Metrics:** Total reward, cumulative regret, adaptability

### ğŸ§  Strategies Compared:
1. Îµ-Greedy (Static)
2. Adaptive Îµ-Greedy
3. UCB
4. Thompson Sampling
5. Smart Thompson Sampling
6. Strategy Switching Meta-Agent

### ğŸ“Š Key Findings:

| Strategy                    | Approx. Total Reward |
|-----------------------------|----------------------|
| Îµ-Greedy (static)           | 7200+                |
| Adaptive Îµ-Greedy           | 7900+                |
| UCB                         | 7800+                |
| Thompson Sampling           | 8400+                |
| Smart Thompson Sampling     | 8500+                |
| Strategy Switching Agent    | 8600+                |

- âœ… **Thompson Sampling** shows superior adaptability and convergence
- âœ… **Smart strategies** better handle dynamic shifts
- âœ… **Meta-agent switching** generalizes well across scenarios

---

## ğŸ§  6. Conclusion

This project highlights:
- Thompson Samplingâ€™s robust, Bayesian approach
- The benefit of adaptive exploration (Îµ decay)
- Real-world relevance: recommender systems, adaptive control, financial systems
- Importance of meta-level reasoning in changing environments

ğŸ“Œ Personally, this project strengthened my skills in:
- Exploration vs. exploitation trade-offs
- Reinforcement learning experimentation
- Algorithm design and evaluation

---

## ğŸ“š 7. References

1. Weng, L. (2018). *Multi-Armed Bandit Algorithms*
2. Agrawal, S., & Goyal, N. (2012). *Analysis of Thompson Sampling*
3. Sutton & Barto (2018). *Reinforcement Learning: An Introduction*
4. DeepMind. *Bandit Learning Video*
5. Farina et al. (2020). *Meta Bandits, NeurIPS*

---

## ğŸ“ Appendix

### ğŸ“Œ Performance Enhancements:
- Adaptive Îµ-decay
- Smart success logic in Thompson Sampling
- Strategy switching based on short-term reward trends

### ğŸ“Œ Literature Contribution:
- Extended classical algorithms
- Compared under non-stationary settings
- Incorporated practical improvements for real-world AI systems

---

## ğŸ’¾ How to Run

1. Install dependencies:
```bash
pip install numpy matplotlib tqdm

#run to notebook
jupyter notebook MultiarmBandit.ipynb
